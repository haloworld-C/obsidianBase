## Core concepts
强化学习的本质是一种最优控制算法。(这么说好像也不对， 因为强化学习是一个更加泛化的框架)
- state 状态(S 状态空间)
- action 动作(A 动作空间)
- reward 奖励(R 奖励空间)
- policy 策略(π 策略空间), 以条件概率进行表达
- value function 价值函数(V 状态价值函数)
- trajectory 轨迹
- episode 回合
- return 回报(对应优化控制中的目标函数Cost)
- bootstrap 递归
### 经典框架
与最优控制的控制框架有惊人的相似性:

### 马尔科夫过程(Markov Process)
#### 马尔科夫性(Markov Property)
马尔科夫性(Markov Property)是马尔科夫过程的最基本特征， 它表示系统的未来状态只依赖于当前状态， 而与过去状态无关。

## 贝尔曼方程
### state-value(cost-to-go)
定义的是从当前状态出发， 到达终止状态的期望奖励(期望代价)
### 贝尔曼公式(Bellman equation)
#### Bellman Equation(deterministic): 
$$
\begin{equation}
\begin{aligned}
v &= r + \gamma P v \\
V_\pi(s) &= \mathbb{E}[R_{t+1} + \gamma V_\pi(S_{t+1}) | S_t = s] \\
\end{aligned}
\end{equation}
\tag{1.1}
$$
> 多条状态轨迹的加权平均值

#### Bellman Equation(stochastic): 
考虑如下state trajectory:
$$
\begin{equation}
\begin{aligned}
S_T \mathop{\rightarrow}\limits^{A_t} R_{t+1},S_{t+1} \mathop{\rightarrow}\limits^{A_{t+1}} \cdots 
\end{aligned}
\end{equation}
\tag{1.2}
$$
计算$G_t$(discounted return):
$$
\begin{equation}
\begin{aligned}
G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \\
&= R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \cdots) \\
&= R_{t+1} + \gamma G_{t+1} \\
\end{aligned}
\end{equation}
\tag{1.3}
$$
定义当前状态价值函数(state-value function):
$$
\begin{equation}
\begin{aligned}
v_\pi(s) &= \mathbb{E}[G_t | S_t = s] \\
&= \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
&= \mathbb{E}[R_{t+1} | S_t = s] + \gamma \mathbb{E}[G_{t+1} | S_t = s] \\
\end{aligned}
\end{equation}
\tag{1.4}
$$
然后我们将上面的式子第一项展开(应用全概率公式):
$$
\begin{equation}
\begin{aligned}
\mathbb{E}[R_t | S_t = s] &= \sum_{a \in A} \pi(a | s) \mathbb{E}[R_{t+1} | S_t = s, A_t = a]\\
&= \sum_{a \in A} \pi(a | s) \sum_{r \in R} p(r|s, a)r \\
\end{aligned}
\end{equation}
\tag{1.5}
$$
然后我们将上面的式子第二项展开:
$$
\begin{equation}
\begin{aligned}
\mathbb{E}[G_t | S_t = s] &= \sum_{s' \in S} \mathbb{E}[G_{t+1} | S_t = s, S_{t+1}=s']p(s'|s)\\
&=\sum_{s' \in S} \mathbb{E}[G_{t+1} | S_{t+1} = s']p(s'|s)\\
&= \sum_{s' \in S} v_{\pi}(s')p(s'|s)\\
&= \sum_{s' \in S} v_{\pi}(s')\sum_{a \in A} \pi(a | s)p(s'|s, a)\\
\end{aligned}
\end{equation}
\tag{1.6}
$$
于是有贝尔曼公式的完全展开形式:
$$
\begin{equation}
\begin{aligned}
V_\pi(s) &= \sum_{a \in A} \pi(a | s) \mathbb{E}[R_{t+1} | S_t = s, A_t = a] + \gamma \sum_{s' \in S} \mathbb{E}[G_{t+1} | S_t = s, S_{t+1}=s']p(s'|s)\\
&= \sum_{a \in A} \pi(a | s) \sum_{r \in R} p(r|s, a)r + \gamma \sum_{s' \in S} v_{\pi}(s')\sum_{a \in A} \pi(a | s)p(s'|s, a)\\
&= \sum_{a \in A} \pi(a | s) \sum_{r \in R} p(r|s, a)r + \gamma \sum_{s' \in S} \sum_{a \in A} \pi(a | s)p(s'|s, a)v_{\pi}(s')\\
&= \sum_{a \in A} \pi(a | s) \sum_{r \in R} p(r|s, a)r + \gamma \sum_{a \in A} \pi(a | s)\sum_{s' \in S} p(s'|s, a)v_{\pi}(s')\\
&= \sum_{a \in A} \pi(a | s)[\sum_{r \in R} p(r|s, a)r + \gamma \sum_{s' \in S} p(s'|s, a)v_{\pi}(s')]
\end{aligned}
\end{equation}
\tag{1.7}
$$
> $\pi(a | s)$是给出的策略， 代表在状态$s$下选择动作$a$的概率
> $p(s'|s, a)$是状态转移概率(也即对象模型)， 代表在状态$s$下选择动作$a$后转移到状态$s'$的概率
#### 贝尔曼公式(矩阵-向量形式)
将式1.7整理为如下形式:
$$
\begin{equation}
\begin{aligned}
V_\pi(s) &= r_{\pi}(s) + \gamma \sum_{s' \in S} p_{\pi}(s'|s)v_{\pi}(s')\\
\end{aligned}
\end{equation}
\tag{1.8}
$$
其中，
$$
\begin{equation}
\begin{aligned}
r_{\pi}(s) &= \sum_{a \in A} \pi(a | s) \sum_{r \in R} p(r|s, a)r \\
p_{\pi}(s'|s) &= \sum_{a \in A} \pi(a | s)p(s'|s, a)
\end{aligned}
\end{equation}
\tag{1.9}
$$
然后将式1.9的下标形式写出来(方便后面整理为矩阵形式):
$$
\begin{equation}
\begin{aligned}
v_{\pi}(s_i)=r_{\pi}(s_i)+\gamma \sum_{s_j \in S} p_{\pi}(s_j | s_i)v_{\pi}(s_j)
\end{aligned}
\end{equation}
\tag{1.10}
$$
然后将所有的$s_i$写为矩阵-向量形式:
$$
\begin{equation}
\begin{aligned}
V_\pi &= r_\pi + \gamma P_{\pi} V_\pi \\
\end{aligned}
\end{equation}
\tag{1.11}
$$
其中，
$$
\begin{equation}
\begin{aligned}
V_{\pi} &= [v_{\pi}(s_1), v_{\pi}(s_2), \cdots, v_{\pi}(s_n)]^T\\
r_{\pi} &= [r_{\pi}(s_1), r_{\pi}(s_2), \cdots, r_{\pi}(s_n)]^T\\
P_{\pi} &\in \mathbb{R}^{n \times n}, [P_{\pi}]_{ij} = p_{\pi}(s_j | s_i), 称为状态转移矩阵
\end{aligned}
\end{equation}
\tag{1.12}
$$
### 贝尔曼公式的求解
#### 解析形式(close-form solution)
对式1.11进行解析求解， 有:
$$
\begin{equation}
\begin{aligned}
V_\pi &= (I - \gamma P_\pi)^{-1} r_\pi
\end{aligned}
\end{equation}
\tag{1.13}
$$
但是这种求解形式需要求逆， 计算代价较大， 一般不使用。
#### 迭代形式(iterative form)
$$
\begin{equation}
\begin{aligned}
V_{\pi}^{(k+1)} &= r_\pi + \gamma P_\pi V_{\pi}^{(k)}
\end{aligned}
\end{equation}
\tag{1.14}
$$
> 可以证明，当k->$\infty$时， $V_{\pi}^{(k)}$收敛到$V_{\pi}$。

证明如下图:
![bellman-solution-proof](../../Resourse/bellman_solution_proof.png)

> 计算state value是为了评判策略的好坏， 进而迭代更好的策略。这个过程便称之为**policy evaluation**

### action value
action value指的是从某个state出发并且采取了某个action之后得到的average return. action value用来选择比较最佳action.
定义:
$$
\begin{equation}
\begin{aligned}
\begin{matrix}\underbrace{q_\pi(s, a)} \\ v_{\pi}(s) \end{matrix} &= \sum_{a \in A}\begin{matrix}\underbrace{\mathbb{E}[G_t | S_t = s, A_t = a]} \\ q_{\pi}(s, a) \end{matrix} \pi(a|s) \\
\end{aligned}
\end{equation}
\tag{1.15}
$$
又由于
$$
\begin{equation}
\begin{aligned}
\mathbb{E}[G_t | S_t = s] &= \mathbb{E}[G_t| S_t = s, A_t = a] \pi(a, s) \\
\end{aligned}
\end{equation}
\tag{1.16}
$$
故有:
$$
\begin{equation}
\begin{aligned}
v_{\pi}(s) &= \sum_{a \in A} \pi(a | s) q_{\pi}(s, a) \\
\end{aligned}
\end{equation}
\tag{1.17}
$$
又由式1.7
$$
\begin{equation}
\begin{aligned}
V_\pi(s) &= \sum_{a \in A} \pi(a | s)\begin{matrix}\underbrace{\sum_{r \in R} p(r|s, a)r + \gamma \sum_{s' \in S} p(s'|s, a)v_{\pi}(s')}\\ q_{\pi}(s, a) \end{matrix}
\end{aligned}
\end{equation}
\tag{1.18}
$$
故，
$$
\begin{equation}
\begin{aligned}
q_{\pi}(s, a) &= \sum_{r \in R} p(r|s, a)r + \gamma \sum_{s' \in S} p(s'|s, a)v_{\pi}(s') \\
\end{aligned}
\end{equation}
\tag{1.19}
$$
## 贝尔曼最优方程Bellman optimality equation(BOE)
### core concepts
#### optimal state value

#### optimal policy
##### 定义
A policy is optimal $\pi^*$ if $V_{\pi^*}(s) = V'(s)$ for all $s \in S$
> 贝尔曼最优方程用来求解optimal policy

##### Bellman optimality equation(intro)
$$
\begin{equation}
\begin{aligned}
v^*(s) &= \max_{\pi} \sum_{a \in A} \pi(a | s) q(s, a) \\
q(s, a) &= \sum_{r \in R} p(r|s, a)r + \gamma \sum_{s' \in S} p(s'|s, a)v(s')
\end{aligned}
\end{equation}
\tag{1.20}
$$
##### Bellman optimality equation(matrix form)
$$
\begin{equation}
\begin{aligned}
v^* &= \max_{\pi} (r_\pi + \gamma P_\pi v) \\
\end{aligned}
\end{equation}
\tag{1.21}
$$
式1.21的求解说明:
![solve optimal policy BOE](../../Resourse/sovle_optimal_BOE_proof.png)
按照上面例子的启发， 我们可以暂时把$v(s')$看做一个已知量，然后得到$\pi^*(s)$的形式表达式， 再看一个例子:
![solve optimal policy BOE example 2](../../Resourse/solve_optimal_policy_BOE_proof2.png)
> 那岂不是最优策略都是确定性的了?

故， 

$$ 
\begin{equation}
\begin{aligned}
\mathop{max}\limits_{\pi} \sum_{a \in A} \pi(a | s) q(s, a) = \mathop{max}\limits_{a \in A} q(s, a)
\end{aligned}
\end{equation}
\tag{1.22}
$$
其中最优策略为:
$$
\begin{equation}
\begin{aligned}
\pi(a|s) &= \left\{ \begin{matrix} 1 & \text{if } a = \arg\max_{a \in A} q(s, a) \\ 0 & \text{otherwise} \end{matrix} \right. \\
\end{aligned}
\end{equation}
\tag{1.23}
$$
$\pi^*$已经求得， 接下来求解$v(s)$
令
$$
\begin{equation}
\begin{aligned}
f(v) := \mathop{max}\limits_{\pi} (r_\pi + \gamma P_\pi v)
\end{aligned}
\end{equation}
\tag{1.24}
$$
那么贝尔曼最优公式变为:
$$
\begin{equation}
\begin{aligned}
v = f(v)
\end{aligned}
\end{equation}
\tag{1.25}
$$
##### Contraction mapping theorem
- fix point:x $\in$ X is a fixed point of f: X -> X if $f(x) = x$
- contraction mapping: 
$$
\begin{equation}
\begin{aligned}
\|f(x) - f(y)\| \leq \alpha \|x - y\|
\end{aligned}
\end{equation}
\tag{1.26}
$$
其中$\alpha \in [0, 1)$, $\|\cdot\|$是向量范数
> 有点李雅普诺夫函数的感觉

- contraction mapping theorem: 
![contraction mapping theorem](../../Resourse/contraction_mapping_theorem.png)

> 看到这个定理， 有点理解状态方程为啥那么写了
##### Contraction property of BOE
对于式1.25中定义的$f(v)$, 可以证明其具有收缩性, 证明见**强化学习的数学原理**这本书(多少页).
于是可以从任意给定初值$v_0$, 通过迭代
$$
\begin{equation}
\begin{aligned}
v_{k+1} &= f(v_k) \\
v^* &= \lim_{k \to \infty} v_k
\end{aligned}
\end{equation}
\tag{1.27}
$$
得到最优状态值$v^*$, 也可以写为如下形式:
$$
\begin{equation}
\begin{aligned}
v^* &= \mathop{max}\limits_{max} (r_\pi + \gamma P_\pi v^*) \\
\end{aligned}
\end{equation}
\tag{1.28}
$$
对于给定的$v^*$, 可以求得:
$$
\begin{equation}
\begin{aligned}
\pi^*(s) &= \arg\max_{\pi} (r_\pi + \gamma P_\pi v^*)\\
\end{aligned}
\end{equation}
\tag{1.29}
$$
再将式1.29代入式1.28, 得到最优策略对应的贝尔曼公式:
$$
\begin{equation}
\begin{aligned}
v^* &= r_{\pi^*} + \gamma P_{\pi^*} v^* \\
\end{aligned}
\end{equation}
\tag{1.30}
$$
其中$\pi^*$的最优性命题如下(证明见书本1):
![optimal policy](../../Resourse/policy_optimality.png)
 
##### 最优策略的形式
![optimal policy form](../../Resourse/greedy_optimal_policy.png)


