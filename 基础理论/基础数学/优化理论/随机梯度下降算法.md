### concept
#### 期望
考虑随机变量$X$， 则$X$的期望定义为:
$$
\begin{equation}
\begin{aligned}
\mathbb{E}[X] &= \sum_{x \in X} x \cdot p(x) \\
\end{aligned}
\end{equation}
\tag{1.1}
$$
如果我们不知道$X$的分布，那么我们可以用样本的平均值来近似$X$的期望(也即蒙特卡洛估计， 服从大数定理)，即:
$$
\begin{equation}
\begin{aligned}
\mathbb{E}[X] &= \frac{1}{N} \sum_{i=1}^{N} x_i \\
\end{aligned}
\end{equation}
\tag{1.2}
$$

当$N\to\infty$时，$\mathbb{E}[X]$收敛于$\mathbb{E}[X]$。

式1.2有两种计算方式:
1. 如果我们采样了所有的$X$，那么带入公式计算即可
2. 如果我们的数据是一帧一帧采样的，那么我们可以用递归的方式获取历史平均， 获取变量的实时估计， 即增量迭代的方法:
对于前k个历史数据的平均值， 定义:
$$
\begin{equation}
\begin{aligned}
w_{k+1} &= \frac{1}{k} \sum_{i=1}^{k} x_i,\space k=1, 2, ...,k \\
\end{aligned}
\end{equation}
\tag{1.3}
$$
显然， 有:
$$
\begin{equation}
\begin{aligned}
w_{k} &= \frac{1}{k-1} \sum_{i=1}^{k-1} x_i,\space k=2, 3, ...\\
\end{aligned}
\end{equation}
\tag{1.4}
$$
故有:
$$
\begin{equation}
\begin{aligned}
w_{k+1} &= \frac{1}{k} \sum_{i=1}^{k} x_i,\space k=1, 2, ...,k \\
&= \frac{1}{k} \left( \sum_{i=1}^{k-1} x_i + x_{k} \right) \\
&= \frac{1}{k} \left( (k-1) \cdot w_k + x_{k} \right) \\
&= \frac{k-1}{k} w_k + \frac{1}{k} x_{k} \\
&=w_k + \frac{1}{k} (x_{k} - w_k) \\
\end{aligned}
\end{equation}
\tag{1.5}
$$
更一般地， 如果把$\frac{1}{k}$替换为系数$\alpha_k$, 则有:
$$
\begin{equation}
\begin{aligned}
w_{k+1} &= w_k + \alpha_k (x_{k} - w_k) \\
\end{aligned}
\end{equation}
\tag{1.6}
$$
> 如果$\alpha_k$满足一些条件， 则$w_k$收敛于$\mathbb{E}[X]$。
#### Stochastic approximation(SA, 随机估计法)
- 一类通过随机采样来逼近未知函数的算法, 经常用于求解方程的解或优化问题。
- 在不知道函数的具体形式的条件下，也无法获取梯度信息，  仍然可以进行求解。

### Robbins-Monro(RM) algorithm
- 随机梯度下降算法是RM的一种特殊情况
#### Problem definition
对于求方程的解的问题:
$$
\begin{equation}
\begin{aligned}
g(w) &= 0 \\
\end{aligned}
\end{equation}
\tag{2.1}
$$
其中$w \in \mathbb{R}$为变量， $g(w): \mathbb{R} \to \mathbb{R}$为函数。

